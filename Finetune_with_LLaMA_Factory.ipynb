{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN/uTOKlqNYR6gnuMpSr1QL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwtan0421/LLaMA-Factory-FT/blob/main/Finetune_with_LLaMA_Factory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tune using Qwen 2.5 - 7B"
      ],
      "metadata": {
        "id": "0LjkRKxeo2PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq"
      ],
      "metadata": {
        "id": "HGkom-0zVAuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJODOmQIOb6j"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6WTn_gCP_wS",
        "outputId": "bc659813-d7b0-4aa5-de4e-c3639b67d89c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Qwen2.5-7B-Instruct'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Total 47 (delta 0), reused 0 (delta 0), pack-reused 47 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (47/47), 3.61 MiB | 8.27 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 14.18 GiB | 101.88 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/jwtan0421/LLaMA-Factory-FT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxDeAUrcAFVr",
        "outputId": "2b4131d8-90f7-4933-d563-373d6f19ce7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory-FT'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 6 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"QWen-2.5\"\n",
        "AUTHOR = \"Osborn\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKkHCoL_UdM1",
        "outputId": "352f985c-b66a-432b-c3ae-a2f1f62964c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/LLaMA-Factory-FT/train_qlora/qwen_lora_sft_bitsandbytes.yaml /content/LLaMA-Factory/examples/train_qlora"
      ],
      "metadata": {
        "id": "qf7bZBp2KG6x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainig\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train examples/train_qlora/qwen_lora_sft_bitsandbytes.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XNP5UbZyUkPd",
        "outputId": "4ebe9cc7-cad2-4278-8abd-debdec1a241b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-11-27 05:49:58.203179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-27 05:49:58.220186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-27 05:49:58.241006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-27 05:49:58.247424: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-27 05:49:58.262404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-27 05:49:59.481820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[WARNING|2024-11-27 05:50:04] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2024-11-27 05:50:04] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 05:50:04,379 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 05:50:04,380 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"/content/Qwen2.5-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,382 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,382 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,382 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,382 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,382 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,382 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 05:50:04,744 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 05:50:04,745 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 05:50:04,746 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"/content/Qwen2.5-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,747 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,747 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,747 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,747 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,747 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 05:50:04,747 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 05:50:05,082 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-11-27 05:50:05] llamafactory.data.template:157 >> Replace eos token: <|im_end|>\n",
            "[INFO|2024-11-27 05:50:05] llamafactory.data.loader:157 >> Loading dataset identity.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 91 examples [00:00, 1585.93 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 91/91 [00:00<00:00, 457.19 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 91/91 [00:02<00:00, 31.31 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 6023, 151645, 198, 151644, 77091, 198, 9707, 0, 358, 1079, 1207, 54, 268, 12, 17, 13, 20, 11, 458, 15235, 17847, 7881, 553, 15433, 15998, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "hi<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Hello! I am QWen-2.5, an AI assistant developed by Osborn. How can I assist you today?<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9707, 0, 358, 1079, 1207, 54, 268, 12, 17, 13, 20, 11, 458, 15235, 17847, 7881, 553, 15433, 15998, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151645]\n",
            "labels:\n",
            "Hello! I am QWen-2.5, an AI assistant developed by Osborn. How can I assist you today?<|im_end|>\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 05:50:09,153 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 05:50:09,155 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"/content/Qwen2.5-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 05:50:09] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3934] 2024-11-27 05:50:09,547 >> loading weights file /content/Qwen2.5-7B-Instruct/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-11-27 05:50:09,547 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 05:50:09,549 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.60s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-11-27 05:50:16,233 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-11-27 05:50:16,233 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /content/Qwen2.5-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1049] 2024-11-27 05:50:16,236 >> loading configuration file /content/Qwen2.5-7B-Instruct/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 05:50:16,237 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 05:50:16] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2024-11-27 05:50:16] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-11-27 05:50:16] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2024-11-27 05:50:16] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2024-11-27 05:50:16] llamafactory.model.model_utils.misc:157 >> Found linear modules: gate_proj,v_proj,up_proj,q_proj,down_proj,o_proj,k_proj\n",
            "[INFO|2024-11-27 05:50:16] llamafactory.model.loader:157 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
            "/content/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(**kwargs)\n",
            "[INFO|trainer.py:698] 2024-11-27 05:50:17,110 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2313] 2024-11-27 05:50:17,474 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2024-11-27 05:50:17,474 >>   Num examples = 81\n",
            "[INFO|trainer.py:2315] 2024-11-27 05:50:17,474 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2024-11-27 05:50:17,474 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2319] 2024-11-27 05:50:17,474 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2320] 2024-11-27 05:50:17,474 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2321] 2024-11-27 05:50:17,474 >>   Total optimization steps = 30\n",
            "[INFO|trainer.py:2322] 2024-11-27 05:50:17,478 >>   Number of trainable parameters = 20,185,088\n",
            "[INFO|integration_utils.py:812] 2024-11-27 05:50:17,487 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20241127_055117-48fd0cku\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaves/qwen/lora/sft\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/osborn-bh-lenovo/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/osborn-bh-lenovo/huggingface/runs/48fd0cku\u001b[0m\n",
            "{'loss': 3.8898, 'grad_norm': 2.87026047706604, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}\n",
            "{'loss': 1.6934, 'grad_norm': 1.6874308586120605, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.98}\n",
            "{'loss': 1.1449, 'grad_norm': 1.602220058441162, 'learning_rate': 0.0, 'epoch': 2.96}\n",
            "100% 30/30 [01:59<00:00,  3.94s/it][INFO|trainer.py:3801] 2024-11-27 05:53:17,584 >> Saving model checkpoint to saves/qwen/lora/sft/checkpoint-30\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 05:53:17,634 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 05:53:17,635 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-11-27 05:53:17,831 >> tokenizer config file saved in saves/qwen/lora/sft/checkpoint-30/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-11-27 05:53:17,831 >> Special tokens file saved in saves/qwen/lora/sft/checkpoint-30/special_tokens_map.json\n",
            "[INFO|trainer.py:2584] 2024-11-27 05:53:18,329 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 180.8512, 'train_samples_per_second': 1.344, 'train_steps_per_second': 0.166, 'train_loss': 2.242702674865723, 'epoch': 2.96}\n",
            "100% 30/30 [02:00<00:00,  4.02s/it]\n",
            "[INFO|trainer.py:3801] 2024-11-27 05:53:18,333 >> Saving model checkpoint to saves/qwen/lora/sft\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 05:53:18,383 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 05:53:18,384 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-11-27 05:53:18,563 >> tokenizer config file saved in saves/qwen/lora/sft/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-11-27 05:53:18,563 >> Special tokens file saved in saves/qwen/lora/sft/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.963\n",
            "  total_flos               =   562012GF\n",
            "  train_loss               =     2.2427\n",
            "  train_runtime            = 0:03:00.85\n",
            "  train_samples_per_second =      1.344\n",
            "  train_steps_per_second   =      0.166\n",
            "Figure saved at: saves/qwen/lora/sft/training_loss.png\n",
            "[WARNING|2024-11-27 05:53:18] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
            "[WARNING|2024-11-27 05:53:18] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4117] 2024-11-27 05:53:18,955 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4119] 2024-11-27 05:53:18,955 >>   Num examples = 10\n",
            "[INFO|trainer.py:4122] 2024-11-27 05:53:18,955 >>   Batch size = 1\n",
            "100% 10/10 [00:01<00:00,  7.92it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      2.963\n",
            "  eval_loss               =      1.135\n",
            "  eval_runtime            = 0:00:01.41\n",
            "  eval_samples_per_second =      7.059\n",
            "  eval_steps_per_second   =      7.059\n",
            "[INFO|modelcard.py:449] 2024-11-27 05:53:20,375 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msaves/qwen/lora/sft\u001b[0m at: \u001b[34mhttps://wandb.ai/osborn-bh-lenovo/huggingface/runs/48fd0cku\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241127_055117-48fd0cku/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/LLaMA-Factory-FT/inference/qwen_lora_sft.yaml /content/LLaMA-Factory/examples/inference"
      ],
      "metadata": {
        "id": "kjppPXiLMa4y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "!llamafactory-cli chat examples/inference/qwen_lora_sft.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_xeuufEIbklK",
        "outputId": "3fb93c6e-bb75-483b-ab31-7312ad4144bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-27 06:01:33.777708: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-27 06:01:33.794810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-27 06:01:33.815608: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-27 06:01:33.821955: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-27 06:01:33.837186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-27 06:01:35.067783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 06:01:39,918 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 06:01:39,919 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"/content/Qwen2.5-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:39,920 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:39,920 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:39,920 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:39,920 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:39,920 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:39,920 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 06:01:40,268 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 06:01:40,269 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 06:01:40,270 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"/content/Qwen2.5-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:40,270 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:40,270 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:40,270 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:40,270 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:40,270 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2209] 2024-11-27 06:01:40,270 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 06:01:40,637 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-11-27 06:01:40] llamafactory.data.template:157 >> Replace eos token: <|im_end|>\n",
            "[INFO|configuration_utils.py:677] 2024-11-27 06:01:40,681 >> loading configuration file /content/Qwen2.5-7B-Instruct/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 06:01:40,682 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"/content/Qwen2.5-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 06:01:40] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3934] 2024-11-27 06:01:40,715 >> loading weights file /content/Qwen2.5-7B-Instruct/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-11-27 06:01:40,716 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 06:01:40,717 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.61s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-11-27 06:01:47,367 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-11-27 06:01:47,367 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /content/Qwen2.5-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1049] 2024-11-27 06:01:47,370 >> loading configuration file /content/Qwen2.5-7B-Instruct/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 06:01:47,371 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 06:01:47] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-11-27 06:01:48] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-11-27 06:01:48] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/qwen/lora/sft\n",
            "[INFO|2024-11-27 06:01:48] llamafactory.model.loader:157 >> all params: 7,615,616,512\n",
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: 你是谁\n",
            "Assistant: 我是 QWen-2.5，由 Osborn 开发的人工智能助手。\n",
            "\n",
            "User: 你擅长什么\n",
            "Assistant: 我可以帮助您解答问题、提供信息、提供建议和解决方案。我还可以帮助您进行一些简单的计算、翻译和生成文本等任务。如果您有任何问题或需要帮助，请随时告诉我。\n",
            "\n",
            "User: 通义前文模型怎么样\n",
            "Assistant: 通义前文模型是 QWen-2.5 的前身，它是我之前的一个版本。通义前文模型经过了训练，能够回答问题、生成文本并提供信息。虽然我已经更新到了 QWen-2.5，但通义前文模型依然具备一定的能力，能够帮助用户解决问题和获取所需的信息。如果您有任何问题或需要帮助，请随时告诉我。\n",
            "\n",
            "User: 你是OpenAI开发的什么？\n",
            "Assistant: 我不是 OpenAI 开发的人工智能助手。我是 QWen-2.5，由 Osborn 开发的人工智能助手。如果您有任何问题或需要帮助，请随时告诉我。\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tune using Llama 3.2 - 3B"
      ],
      "metadata": {
        "id": "tZEylqBJpSkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnSZxssIp0ZO",
        "outputId": "2c30cbd1-52d8-4c0a-cd70-1cb08db685d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "The token `Colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Colab`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainig\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train examples/train_qlora/llama3.2_lora_sft_awq.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yRVQJCV3wk00",
        "outputId": "4e24da4d-bd99-4e6d-d8d0-42b2480169ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-11-27 09:02:10.464341: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-27 09:02:10.481657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-27 09:02:10.502900: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-27 09:02:10.509342: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-27 09:02:10.524543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-27 09:02:11.779802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[WARNING|2024-11-27 09:02:16] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2024-11-27 09:02:16] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:02:16,820 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:02:16,821 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:16,912 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:16,912 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:16,912 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:16,912 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:16,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 09:02:17,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:02:17,729 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:02:17,730 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:17,802 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:17,802 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:17,803 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:17,803 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:02:17,803 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 09:02:18,271 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-11-27 09:02:18] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n",
            "[INFO|2024-11-27 09:02:18] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
            "[INFO|2024-11-27 09:02:18] llamafactory.data.loader:157 >> Loading dataset identity.json...\n",
            "Converting format of dataset (num_proc=16): 100% 91/91 [00:00<00:00, 395.65 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 91/91 [00:04<00:00, 20.58 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 1229, 54, 268, 12, 17, 13, 20, 11, 459, 15592, 18328, 8040, 555, 15796, 16381, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am QWen-2.5, an AI assistant developed by Osborn. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 1229, 54, 268, 12, 17, 13, 20, 11, 459, 15592, 18328, 8040, 555, 15796, 16381, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am QWen-2.5, an AI assistant developed by Osborn. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:02:24,063 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:02:24,064 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 09:02:24] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3937] 2024-11-27 09:02:24,209 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-11-27 09:02:24,210 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 09:02:24,212 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.44s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-11-27 09:02:27,304 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-11-27 09:02:27,304 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-11-27 09:02:27,384 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 09:02:27,384 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 09:02:27] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2024-11-27 09:02:27] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-11-27 09:02:27] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2024-11-27 09:02:27] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2024-11-27 09:02:27] llamafactory.model.model_utils.misc:157 >> Found linear modules: up_proj,gate_proj,v_proj,q_proj,o_proj,down_proj,k_proj\n",
            "[INFO|2024-11-27 09:02:27] llamafactory.model.loader:157 >> trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n",
            "/content/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(**kwargs)\n",
            "[INFO|trainer.py:698] 2024-11-27 09:02:28,024 >> Using auto half precision backend\n",
            "[WARNING|2024-11-27 09:02:28] llamafactory.train.callbacks:168 >> Previous trainer log in this folder will be deleted.\n",
            "[INFO|trainer.py:2313] 2024-11-27 09:02:28,404 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2024-11-27 09:02:28,404 >>   Num examples = 81\n",
            "[INFO|trainer.py:2315] 2024-11-27 09:02:28,404 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2024-11-27 09:02:28,404 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2319] 2024-11-27 09:02:28,404 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2320] 2024-11-27 09:02:28,404 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2321] 2024-11-27 09:02:28,404 >>   Total optimization steps = 30\n",
            "[INFO|trainer.py:2322] 2024-11-27 09:02:28,408 >>   Number of trainable parameters = 12,156,928\n",
            "[INFO|integration_utils.py:812] 2024-11-27 09:02:28,417 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mosborn-bh\u001b[0m (\u001b[33mosborn-bh-lenovo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20241127_090228-20gmpsqd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaves/llama3.2-8b/lora/sft\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/osborn-bh-lenovo/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/osborn-bh-lenovo/huggingface/runs/20gmpsqd\u001b[0m\n",
            "{'loss': 3.0861, 'grad_norm': 2.611318826675415, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}\n",
            "{'loss': 1.8656, 'grad_norm': 1.8943283557891846, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.98}\n",
            "{'loss': 1.3474, 'grad_norm': 1.954235553741455, 'learning_rate': 0.0, 'epoch': 2.96}\n",
            "100% 30/30 [01:58<00:00,  3.90s/it][INFO|trainer.py:3801] 2024-11-27 09:04:28,074 >> Saving model checkpoint to saves/llama3.2-8b/lora/sft/checkpoint-30\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:04:28,261 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:04:28,262 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-11-27 09:04:28,451 >> tokenizer config file saved in saves/llama3.2-8b/lora/sft/checkpoint-30/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-11-27 09:04:28,451 >> Special tokens file saved in saves/llama3.2-8b/lora/sft/checkpoint-30/special_tokens_map.json\n",
            "[INFO|trainer.py:2584] 2024-11-27 09:04:28,962 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 120.5538, 'train_samples_per_second': 2.016, 'train_steps_per_second': 0.249, 'train_loss': 2.099704138437907, 'epoch': 2.96}\n",
            "100% 30/30 [01:59<00:00,  3.98s/it]\n",
            "[INFO|trainer.py:3801] 2024-11-27 09:04:28,966 >> Saving model checkpoint to saves/llama3.2-8b/lora/sft\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:04:29,153 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:04:29,154 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-11-27 09:04:29,298 >> tokenizer config file saved in saves/llama3.2-8b/lora/sft/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-11-27 09:04:29,298 >> Special tokens file saved in saves/llama3.2-8b/lora/sft/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.963\n",
            "  total_flos               =   213998GF\n",
            "  train_loss               =     2.0997\n",
            "  train_runtime            = 0:02:00.55\n",
            "  train_samples_per_second =      2.016\n",
            "  train_steps_per_second   =      0.249\n",
            "Figure saved at: saves/llama3.2-8b/lora/sft/training_loss.png\n",
            "[WARNING|2024-11-27 09:04:29] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
            "[WARNING|2024-11-27 09:04:29] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4117] 2024-11-27 09:04:29,715 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4119] 2024-11-27 09:04:29,715 >>   Num examples = 10\n",
            "[INFO|trainer.py:4122] 2024-11-27 09:04:29,715 >>   Batch size = 1\n",
            "100% 10/10 [00:01<00:00,  7.96it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      2.963\n",
            "  eval_loss               =     1.3414\n",
            "  eval_runtime            = 0:00:01.40\n",
            "  eval_samples_per_second =        7.1\n",
            "  eval_steps_per_second   =        7.1\n",
            "[INFO|modelcard.py:449] 2024-11-27 09:04:31,129 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msaves/llama3.2-8b/lora/sft\u001b[0m at: \u001b[34mhttps://wandb.ai/osborn-bh-lenovo/huggingface/runs/20gmpsqd\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241127_090228-20gmpsqd/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9PH2wvSvcmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli chat examples/inference/llama3.2_lora_sft.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aLDK3idupDS",
        "outputId": "1c478030-3e95-4cd0-828b-07f24d9e3222"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-11-27 09:18:58.800533: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-27 09:18:58.817728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-27 09:18:58.838814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-27 09:18:58.845232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-27 09:18:58.860500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-27 09:19:00.099948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:19:05,143 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:19:05,145 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:05,260 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:05,260 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:05,260 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:05,260 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:05,260 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 09:19:05,743 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:19:06,093 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:19:06,094 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:06,197 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:06,197 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:06,197 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:06,197 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-11-27 09:19:06,198 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-11-27 09:19:06,662 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-11-27 09:19:06] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n",
            "[INFO|2024-11-27 09:19:06] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:679] 2024-11-27 09:19:06,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-11-27 09:19:06,772 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 09:19:06] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-11-27 09:19:06,806 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-11-27 09:19:06,807 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 09:19:06,808 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.26s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-11-27 09:19:09,523 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-11-27 09:19:09,523 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-11-27 09:19:09,694 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-11-27 09:19:09,695 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-11-27 09:19:09] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-11-27 09:19:10] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-11-27 09:19:10] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/llama3.2-8b/lora/sft\n",
            "[INFO|2024-11-27 09:19:10] llamafactory.model.loader:157 >> all params: 3,212,749,824\n",
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: 你是谁\n",
            "Assistant: 我是 QWen-2.5，来自 Osborn-2.5。我的主要功能是回答用户的提问和完成各种任务。\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/LLaMA-Factory/saves/llama3.2-8b/lora/sft"
      ],
      "metadata": {
        "id": "d28vThJt6jMF"
      }
    }
  ]
}